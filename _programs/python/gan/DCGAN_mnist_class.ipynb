{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUP8YUdF6oQ8",
        "outputId": "ea1655b6-cb58-4588-b603-95f10c53c7e5"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージのインストール\n",
        "!pip install torch torchvision\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrivS6eI6xrk",
        "outputId": "a5ee7084-d738-4caa-8129-5063e88c247b"
      },
      "outputs": [],
      "source": [
        "# Google Driveとのデータのやり取り\n",
        "from google.colab import drive\n",
        "drive_dir = '/content/drive'\n",
        "drive.mount(drive_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8BQRWv965Lz"
      },
      "outputs": [],
      "source": [
        "# パッケージのインストール\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import struct\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSTggVFr7F7T"
      },
      "outputs": [],
      "source": [
        "# よく使うファイル・ディレクトリ\n",
        "colab_dir = os.path.join(drive_dir, 'My Drive', 'Colab Notebooks')\n",
        "model_path = os.path.join(colab_dir, 'DCGAN_mnist.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywD8U9hy7KOD"
      },
      "outputs": [],
      "source": [
        "# データセットの読み取り\n",
        "class MnistDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, mode='train'):\n",
        "        super(MnistDataset, self).__init__()\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.mode = mode\n",
        "        self.n_classes = 10\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.image_file = 'train-images-idx3-ubyte'\n",
        "            self.label_file = 'train-labels-idx1-ubyte'\n",
        "        elif self.mode == 'test':\n",
        "            self.image_file = 't10k-images-idx3-ubyte'\n",
        "            self.label_file = 't10k-labels-idx1-ubyte'\n",
        "        else:\n",
        "            raise Exception('MNIST dataset mode must be \"train\" or \"test\"')\n",
        "        \n",
        "        self.image_data = self._load_images(os.path.join(self.root_dir, self.image_file))\n",
        "        self.label_data = self._load_labels(os.path.join(self.root_dir, self.label_file))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'images': self.image_data[idx],\n",
        "            'labels': self.label_data[idx]\n",
        "        }\n",
        "\n",
        "    def _load_images(self, filename):\n",
        "        with open(filename, 'rb') as fp:\n",
        "            magic = struct.unpack('>i', fp.read(4))[0]\n",
        "            if magic != 2051:\n",
        "                raise Exception('Magic number does not match!')\n",
        "\n",
        "            n_images, height, width = struct.unpack('>iii', fp.read(4 * 3))\n",
        "\n",
        "            n_pixels = n_images * height * width\n",
        "            pixels = struct.unpack('>' + 'B' * n_pixels, fp.read(n_pixels))\n",
        "            pixels = np.asarray(pixels, dtype='uint8').reshape((n_images, 1, height, width))\n",
        "\n",
        "            # 画像サイズを2べきにしておく\n",
        "            pixels = np.pad(pixels, [(0, 0), (0, 0), (2, 2), (2, 2)], mode='constant', constant_values=0)\n",
        "            pixels = (pixels / 255.0).astype('float32')\n",
        "\n",
        "        return pixels\n",
        "\n",
        "    def _load_labels(self, filename):\n",
        "        with open(filename, 'rb') as fp:\n",
        "            magic = struct.unpack('>i', fp.read(4))[0]\n",
        "            if magic != 2049:\n",
        "                raise Exception('Magic number does not match!')\n",
        "\n",
        "            n_labels = struct.unpack('>i', fp.read(4))[0]\n",
        "            labels = struct.unpack('>' + 'B' * n_labels, fp.read(n_labels))\n",
        "            labels = np.asarray(labels, dtype='int64')\n",
        "\n",
        "        return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LtSv3_-IHF4"
      },
      "outputs": [],
      "source": [
        "# 基本処理\n",
        "class BlockG(nn.Module):\n",
        "    \"\"\" Basic convolution block for generator (Conv, BN, ReLU) \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(BlockG, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class BlockD(nn.Module):\n",
        "    \"\"\" Basic convolution block for discriminator (Conv, LeakyReLU) \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(BlockD, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\" Up-sampling \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Up, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\" Down-sampling \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQPNQ1GEH1TU"
      },
      "outputs": [],
      "source": [
        "# Generatorの定義\n",
        "class NetG(nn.Module):\n",
        "    def __init__(self, in_features=128, out_channels=3, base_filters=8):\n",
        "        super(NetG, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_channels = out_channels\n",
        "        self.base_filters = base_filters\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.in_features, self.base_filters * 8,\n",
        "                                kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(self.base_filters * 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Up(self.base_filters * 8, self.base_filters * 4),\n",
        "            Up(self.base_filters * 4, self.base_filters * 2),\n",
        "            Up(self.base_filters * 2, self.base_filters * 1),\n",
        "            nn.Conv2d(base_filters * 1, self.out_channels,\n",
        "                      kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_batches, n_dims = x.size()\n",
        "        x = x.view(n_batches, n_dims, 1, 1)\n",
        "        for conv in self.net:\n",
        "            x = conv(x)\n",
        "        return torch.tanh(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oTTmN6eKhW8"
      },
      "outputs": [],
      "source": [
        "# Discriminatorの定義\n",
        "class NetD(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_filters=8):\n",
        "        super(NetD, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.base_filters = base_filters\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            BlockD(self.in_channels, self.base_filters),\n",
        "            Down(self.base_filters, self.base_filters * 2),\n",
        "            Down(self.base_filters * 2, self.base_filters * 4),\n",
        "            Down(self.base_filters * 4, self.base_filters * 8),\n",
        "            nn.Conv2d(self.base_filters * 8, 1, kernel_size=4, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x.squeeze()  # BCELossWithLogitsを使うのでsigmoidに入れない"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6E1xjTYtaEy"
      },
      "outputs": [],
      "source": [
        "def to_onehot(cls, n_classes):\n",
        "    ident = torch.eye(n_classes, dtype=torch.float32, device=cls.device)\n",
        "    return ident[cls]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFNb5_oJNZkx",
        "outputId": "8832b6aa-cae4-4191-af3e-16239097ca56"
      },
      "outputs": [],
      "source": [
        "# 使用するデバイスの設定\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda', 0)\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print('Device: {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8XhJGjPOxsf"
      },
      "outputs": [],
      "source": [
        "# 各種パラメータ\n",
        "sample_dims = 32            # zの次元\n",
        "base_lr = 2.0e-4            # 学習率\n",
        "beta1 = 0.5                 # Adamのbeta1\n",
        "beta2 = 0.9                 # Adamのbeta2\n",
        "base_filters = 8            # CNNの基本チャンネル数\n",
        "data_root = 'MNIST'  # データセットのディレクトリ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVkaDYaCOimi",
        "outputId": "939f3c57-7163-4e2a-ff05-fcfbfe97b2ab"
      },
      "outputs": [],
      "source": [
        "# データセットローダの準備\n",
        "dataset = MnistDataset(os.path.join(colab_dir, data_root))\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=25, num_workers=4, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OM0m9QqNe_j"
      },
      "outputs": [],
      "source": [
        "# ネットワークとoptimizerの定義\n",
        "n_classes = dataset.n_classes\n",
        "netD = NetD(in_channels=1 + n_classes, base_filters=base_filters)\n",
        "netG = NetG(in_features=sample_dims + n_classes, out_channels=1, base_filters=base_filters)\n",
        "netD.to(device)\n",
        "netG.to(device)\n",
        "optimD = torch.optim.Adam(netD.parameters(), lr=base_lr, betas=(beta1, beta2))\n",
        "optimG = torch.optim.Adam(netG.parameters(), lr=base_lr, betas=(beta1, beta2))\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# モデルファイルの読み込み (続きから学習するときはreload_modelをTrueにする)\n",
        "reload_model = False\n",
        "start_epoch = 0\n",
        "start_steps = 0\n",
        "if reload_model:\n",
        "    ckpt = torch.load(model_path)\n",
        "    optimG.load_state_dict(ckpt['optimG'])\n",
        "    optimD.load_state_dict(ckpt['optimD'])\n",
        "    netG.load_state_dict(ckpt['netG'])\n",
        "    netD.load_state_dict(ckpt['netD'])\n",
        "    start_epoch = ckpt['epoch']\n",
        "    start_steps = ckpt['steps']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMS-rz0GOFPS"
      },
      "outputs": [],
      "source": [
        "# 学習の途中経過を保存するフォルダの作成\n",
        "now = datetime.datetime.now()\n",
        "time_stamp = now.strftime('%Y%m%d-%H%M%S')\n",
        "runs_dir = os.path.join(colab_dir, 'runs')\n",
        "log_dir = os.path.join(runs_dir, time_stamp)\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvDEdUxsPPhy",
        "outputId": "a13b544b-69d6-4d27-ec23-b335ac04d015"
      },
      "outputs": [],
      "source": [
        "# 学習ループ\n",
        "steps = start_steps\n",
        "for epoch in range(start_epoch, 100):\n",
        "    tqdm_iter = tqdm(data_loader, file=sys.stdout)\n",
        "    for data in tqdm_iter:\n",
        "        x_real = data['images'].to(device)\n",
        "        c_real = data['labels'].to(device)\n",
        "        n_batches, _, height, width = x_real.size()\n",
        "\n",
        "        x_real = 2.0 * x_real - 1.0\n",
        "        c_onehot = to_onehot(c_real, n_classes)\n",
        "        c_onehot_tile = c_onehot.view(n_batches, -1, 1, 1).repeat(1, 1, height, width)\n",
        "\n",
        "        netD.train()\n",
        "        netG.train()\n",
        "\n",
        "        # Discriminatorの学習\n",
        "        optimD.zero_grad()\n",
        "\n",
        "        z = torch.randn([n_batches, sample_dims], dtype=torch.float32, device=device)\n",
        "        z = torch.cat([z, c_onehot], dim=1)\n",
        "        x_fake = netG(z)\n",
        "        x_fake = x_fake.detach()\n",
        "        y_fake = netD(torch.cat([x_fake, c_onehot_tile], dim=1))\n",
        "        y_real = netD(torch.cat([x_real, c_onehot_tile], dim=1))\n",
        "\n",
        "        lossD = criterion(y_fake, torch.zeros_like(y_fake)) +\\\n",
        "                criterion(y_real, torch.ones_like(y_real))\n",
        "        lossD.backward()\n",
        "\n",
        "        optimD.step()\n",
        "\n",
        "        # Generatorの学習\n",
        "        optimG.zero_grad()\n",
        "\n",
        "        z = torch.randn([n_batches, sample_dims], dtype=torch.float32, device=device)\n",
        "        z = torch.cat([z, c_onehot], dim=1)\n",
        "        x_fake = netG(z)\n",
        "        y_fake = netD(torch.cat([x_fake, c_onehot_tile], dim=1))\n",
        "\n",
        "        lossG = criterion(y_fake, torch.ones_like(y_fake))\n",
        "        lossG.backward()\n",
        "\n",
        "        optimG.step()\n",
        "\n",
        "        # ロスを標準出力する\n",
        "        tqdm_iter.set_description(\"epoch #{:d}, {:d} steps, lossD={:.4f}, lossG={:.4f}\".format(epoch, steps, lossD.item(), lossG.item()))\n",
        "\n",
        "        # 途中経過の保存\n",
        "        if steps % 50 == 0:\n",
        "            outfile = os.path.join(log_dir, 'x_real_{:03d}.jpg'.format(epoch))\n",
        "            torchvision.utils.save_image(x_real * 0.5 + 0.5, outfile, nrow=5, padding=10)\n",
        "            outfile = os.path.join(log_dir, 'x_fake_{:03d}.jpg'.format(epoch))\n",
        "            torchvision.utils.save_image(x_fake * 0.5 + 0.5, outfile, nrow=5, padding=10)\n",
        "\n",
        "            netG.eval()\n",
        "            z = torch.randn([n_classes, sample_dims], dtype=torch.float32).to(device)\n",
        "            c_onehot = torch.eye(n_classes, dtype=torch.float32).to(device)\n",
        "            z = torch.cat([z, c_onehot], dim=1)\n",
        "            x_fake = netG(z)\n",
        "            \n",
        "            outfile = os.path.join(log_dir, 'x_fake_class_{:03d}.jpg'.format(epoch))\n",
        "            torchvision.utils.save_image(x_fake * 0.5 + 0.5, outfile, nrow=5, padding=10)\n",
        "\n",
        "\n",
        "        steps += 1\n",
        "\n",
        "    # 学習途中のモデルを保存\n",
        "    ckpt = {\n",
        "        'optimG': optimG.state_dict(),\n",
        "        'optimD': optimD.state_dict(),\n",
        "        'netG': netG.state_dict(),\n",
        "        'netD': netD.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'steps': steps\n",
        "    }\n",
        "    torch.save(ckpt, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "plJ85-yWRgFJ"
      },
      "outputs": [],
      "source": [
        "# 10x10の画像を作る\n",
        "n_samples = 10\n",
        "\n",
        "netG.eval()\n",
        "\n",
        "z = torch.randn([n_samples, sample_dims], dtype=torch.float32, device=device)\n",
        "z = z.repeat(1, n_classes).view(-1, sample_dims)\n",
        "c_onehot = torch.eye(n_classes, dtype=torch.float32).to(device)\n",
        "c_onehot = c_onehot.repeat(n_samples, 1)\n",
        "\n",
        "z_and_c = torch.cat([z, c_onehot], dim=1)\n",
        "x_fake = netG(z_and_c)\n",
        "\n",
        "outfile = os.path.join(log_dir, 'x_fake_class_tile.jpg'.format(epoch))\n",
        "torchvision.utils.save_image(x_fake * 0.5 + 0.5, outfile, nrow=n_samples, padding=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p-6j4tmPVjCW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DCGAN_mnist_class.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
