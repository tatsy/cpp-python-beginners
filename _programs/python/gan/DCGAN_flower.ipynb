{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_flower.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUP8YUdF6oQ8"
      },
      "source": [
        "# 必要なパッケージのインストール\n",
        "!pip install torch torchvision\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrivS6eI6xrk"
      },
      "source": [
        "# Google Driveとのデータのやり取り\n",
        "from google.colab import drive\n",
        "drive_dir = '/content/drive'\n",
        "drive.mount(drive_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw8IciRFjEJr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8BQRWv965Lz"
      },
      "source": [
        "# パッケージのインストール\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import struct\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSTggVFr7F7T"
      },
      "source": [
        "# よく使うファイル・ディレクトリ\n",
        "colab_dir = os.path.join(drive_dir, 'My Drive', 'Colab Notebooks')\n",
        "model_path = 'DCGAN_flower.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywD8U9hy7KOD"
      },
      "source": [
        "# データセットの読み取り\n",
        "class OxfordFlowerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, resize=128):\n",
        "        super(OxfordFlowerDataset, self).__init__()\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.resize = resize\n",
        "        self.image_list = [f for f in os.listdir(self.root_dir) if not f.startswith('.jpg')]\n",
        "        self.image_list = [os.path.join(self.root_dir, f) for f in self.image_list]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 画像の読み込み\n",
        "        image_file = self.image_list[idx]\n",
        "        image = cv2.imread(image_file, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # 画像のクロップ\n",
        "        h, w, _ = image.shape\n",
        "        size = min(h, w)\n",
        "        cx, cy = w // 2, h // 2\n",
        "        sx = max(0, cx - size // 2)\n",
        "        sy = max(0, cy - size // 2)\n",
        "        image = image[sy:sy+size, sx:sx+size, :]\n",
        "\n",
        "        image = cv2.resize(image, (self.resize, self.resize))\n",
        "        image = (image / 255.0).astype('float32')\n",
        "        image = np.transpose(image, axes=(2, 0, 1))\n",
        "        return {\n",
        "            'images': image\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LtSv3_-IHF4"
      },
      "source": [
        "# 基本処理\n",
        "class BlockG(nn.Module):\n",
        "    \"\"\" Basic convolution block for generator (Conv, BN, ReLU) \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(BlockG, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class BlockD(nn.Module):\n",
        "    \"\"\" Basic convolution block for discriminator (Conv, LeakyReLU) \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(BlockD, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\" Up-sampling \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Up, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\" Down-sampling \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQPNQ1GEH1TU"
      },
      "source": [
        "# Generatorの定義\n",
        "class NetG(nn.Module):\n",
        "    def __init__(self, in_features=128, out_channels=3, base_filters=8):\n",
        "        super(NetG, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_channels = out_channels\n",
        "        self.base_filters = base_filters\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.in_features, self.base_filters * 16,\n",
        "                                kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(self.base_filters * 16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Up(self.base_filters * 16, self.base_filters * 16),\n",
        "            Up(self.base_filters * 16, self.base_filters * 8),\n",
        "            Up(self.base_filters * 8, self.base_filters * 4),\n",
        "            Up(self.base_filters * 4, self.base_filters * 2),\n",
        "            Up(self.base_filters * 2, self.base_filters * 1),\n",
        "            nn.Conv2d(base_filters * 1, self.out_channels,\n",
        "                      kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_batches, n_dims = x.size()\n",
        "        x = x.view(n_batches, n_dims, 1, 1)\n",
        "        x = self.net(x)\n",
        "        return torch.tanh(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oTTmN6eKhW8"
      },
      "source": [
        "# Discriminatorの定義\n",
        "class NetD(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_filters=8):\n",
        "        super(NetD, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.base_filters = base_filters\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            BlockD(self.in_channels, self.base_filters),\n",
        "            Down(self.base_filters, self.base_filters * 2),\n",
        "            Down(self.base_filters * 2, self.base_filters * 4),\n",
        "            Down(self.base_filters * 4, self.base_filters * 8),\n",
        "            Down(self.base_filters * 8, self.base_filters * 16),\n",
        "            Down(self.base_filters * 16, self.base_filters * 16),\n",
        "            nn.Conv2d(self.base_filters * 16, 1, kernel_size=4, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x.squeeze()  # BCELossWithLogitsを使うのでsigmoidに入れない"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFNb5_oJNZkx"
      },
      "source": [
        "# 使用するデバイスの設定\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda', 0)\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8XhJGjPOxsf"
      },
      "source": [
        "# 各種パラメータ\n",
        "sample_dims = 32            # zの次元\n",
        "base_lr = 2.0e-4            # 学習率\n",
        "beta1 = 0.5                 # Adamのbeta1\n",
        "base_filters = 32           # CNNの基本チャンネル数\n",
        "data_root = 'OxfordFlower'  # データセットのディレクトリ\n",
        "total_epochs = 20           # 総学習エポック数 (適宜増やす)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OM0m9QqNe_j"
      },
      "source": [
        "# ネットワークとoptimizerの定義\n",
        "netD = NetD(in_channels=3, base_filters=base_filters)\n",
        "netG = NetG(in_features=sample_dims, base_filters=base_filters)\n",
        "netD.to(device)\n",
        "netG.to(device)\n",
        "optimD = torch.optim.Adam(netD.parameters(), lr=base_lr, betas=(beta1, 0.999))\n",
        "optimG = torch.optim.Adam(netG.parameters(), lr=base_lr, betas=(beta1, 0.999))\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# モデルファイルの読み込み (続きから学習するときはresumeにフォルダ名を入れる)\n",
        "resume = ''\n",
        "start_epoch = 0\n",
        "start_steps = 0\n",
        "if resume != '':\n",
        "    # 保存済みモデルから読み込み\n",
        "    log_dir = os.path.join(colab_dir, 'runs', resume)\n",
        "    ckpt = torch.load(os.path.join(log_dir, model_path))\n",
        "    optimG.load_state_dict(ckpt['optimG'])\n",
        "    optimD.load_state_dict(ckpt['optimD'])\n",
        "    netG.load_state_dict(ckpt['netG'])\n",
        "    netD.load_state_dict(ckpt['netD'])\n",
        "    start_epoch = ckpt['epoch'] + 1\n",
        "    start_steps = ckpt['steps']\n",
        "else:\n",
        "    # 学習の途中経過を保存するフォルダの作成\n",
        "    now = datetime.datetime.now()\n",
        "    time_stamp = now.strftime('%Y%m%d-%H%M%S')\n",
        "    runs_dir = os.path.join(colab_dir, 'runs')\n",
        "    log_dir = os.path.join(runs_dir, time_stamp)\n",
        "    os.makedirs(log_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkaDYaCOimi"
      },
      "source": [
        "# データセットローダの準備\n",
        "dataset = OxfordFlowerDataset(os.path.join(colab_dir, data_root), resize=128)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=25, num_workers=4, shuffle=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvDEdUxsPPhy"
      },
      "source": [
        "# 学習ループ\n",
        "steps = start_steps\n",
        "for epoch in range(start_epoch, 100):\n",
        "    tqdm_iter = tqdm(data_loader, file=sys.stdout)\n",
        "    for data in tqdm_iter:\n",
        "        x_real = data['images'].to(device)\n",
        "        x_real = 2.0 * x_real - 1.0\n",
        "        n_batches, _, _, _ = x_real.size()\n",
        "\n",
        "        netD.train()\n",
        "        netG.train()\n",
        "\n",
        "        # Discriminatorの学習\n",
        "        optimD.zero_grad()\n",
        "\n",
        "        z = torch.randn([n_batches, sample_dims], dtype=torch.float32, device=device)\n",
        "        x_fake = netG(z)\n",
        "        x_fake = x_fake.detach()\n",
        "        y_fake = netD(x_fake)\n",
        "        y_real = netD(x_real)\n",
        "\n",
        "        lossD = criterion(y_fake, torch.zeros_like(y_fake)) +\\\n",
        "                criterion(y_real, torch.ones_like(y_real))\n",
        "        lossD.backward()\n",
        "\n",
        "        optimD.step()\n",
        "\n",
        "        # Generatorの学習\n",
        "        optimG.zero_grad()\n",
        "\n",
        "        z = torch.randn([n_batches, sample_dims], dtype=torch.float32, device=device)\n",
        "        x_fake = netG(z)\n",
        "        y_fake = netD(x_fake)\n",
        "\n",
        "        lossG = criterion(y_fake, torch.ones_like(y_fake))\n",
        "        lossG.backward()\n",
        "\n",
        "        optimG.step()\n",
        "\n",
        "        # ロスを標準出力する\n",
        "        tqdm_iter.set_description(\"epoch #{:d}, {:d} steps, lossD={:.4f}, lossG={:.4f}\".format(epoch, steps, lossD.item(), lossG.item()))\n",
        "\n",
        "        # 途中経過の保存\n",
        "        if steps % 50 == 0:\n",
        "            outfile = os.path.join(log_dir, 'x_real_{:03d}.jpg'.format(epoch))\n",
        "            torchvision.utils.save_image(x_real * 0.5 + 0.5, outfile, nrow=5, padding=10)\n",
        "            outfile = os.path.join(log_dir, 'x_fake_{:03d}.jpg'.format(epoch))\n",
        "            torchvision.utils.save_image(x_fake * 0.5 + 0.5, outfile, nrow=5, padding=10)\n",
        "\n",
        "        steps += 1\n",
        "\n",
        "    # 学習途中のモデルを保存\n",
        "    ckpt = {\n",
        "        'optimG': optimG.state_dict(),\n",
        "        'optimD': optimD.state_dict(),\n",
        "        'netG': netG.state_dict(),\n",
        "        'netD': netD.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'steps': steps\n",
        "    }\n",
        "    torch.save(ckpt, os.path.join(log_dir, model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plJ85-yWRgFJ"
      },
      "source": [
        "# 10x10の画像を作る\n",
        "rows = 10\n",
        "cols = 10\n",
        "netG.eval()\n",
        "\n",
        "z = torch.randn([rows * cols, sample_dims], dtype=torch.float32, device=device)\n",
        "x_fake = netG(z)\n",
        "\n",
        "image_grid = torchvision.utils.make_grid(x_fake * 0.5 + 0.5, nrow=rows, padding=10)\n",
        "image_grid = image_grid.detach().cpu().numpy()\n",
        "image_grid = np.transpose(image_grid, axes=[1, 2, 0])\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(image_grid)\n",
        "plt.show()\n",
        "\n",
        "# 保存するときは以下をコメントアウト(適宜保存する名前は変更すること)\n",
        "# cv2.imwrite('image_grid_10x10.png', image_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-6j4tmPVjCW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}